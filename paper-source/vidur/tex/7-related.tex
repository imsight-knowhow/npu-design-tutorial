\section{Related Work}
\label{sec-related}

Prior techniques leverage the predictability of DNN training iterations~\cite{astra, gandiva} to model the performance of the entire job. For example, Habitat~\cite{habitat} models the performance of a training job on different types of GPUs based on the runtime profile collected of a few training iterations on a given GPU. In doing so, Habitat applies the roofline model~\cite{roofline} to estimate the performance of individual operators based on the compute and memory requirements of the operator along with the compute and memory bandwidth of a GPU. Daydream~\cite{daydream} proposes a different approach focused on modeling the effect of various system optimizations on training performance across various deployment scenarios. Daydream can help answer questions like: what is the main performance bottleneck in my training job (e.g., memory or network bandwidth), how will optimizations like kernel-fusion, quantization or gradient compression help improve performance etc.  To accurately model the effect of such optimizations, Daydream first constructs a computation graph of a training job and then applies optimizations via graph transformations (e.g., kernel-fusion can be applied by substituting individual kernel nodes with a single node that represents the fused kernels in the computation graph). Proteus~\cite{proteus} further enables simulating various parallelization strategies to identify the best partitioning and scheduling strategy for a given training job. It does so by first modeling a parallelization strategy with a unified representation called \textit{Strategy Tree} and then compiling it into a distributed execution graph. In another approach ~\cite{recommender_modeling}, the authors propose a critical-path based strategy to predict the per-batch training time of deep learning recommendation models. Different from these training-based simulators, \sysname is the first simulator that accounts for the specific properties of LLM inference.





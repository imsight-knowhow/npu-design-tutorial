# GPU Design Evaluation for DNN Inference: Methods and Tools

Evaluating new GPU architectures for deep learning inference – whether for large language models (LLMs), convolutional neural networks (CNNs), or vision transformers (ViTs) – requires a combination of analytical modeling and simulation-based techniques. This survey outlines the key approaches (from first-order theoretical models to detailed simulators) and highlights notable open-source tools (both academic and industrial-origin) used to assess inference performance, identify bottlenecks, and explore architectural trade-offs in GPU design.

## Analytical Models and Theoretical Predictions

Analytical performance models provide quick, first-order estimates of how a proposed GPU design might handle DNN inference workloads. These models use mathematical formulas or simplified abstractions rather than running full simulations. For example, **roofline analysis** is a popular method that plots an application’s *arithmetic intensity* (operations per byte) against hardware performance ceilings (peak FLOPS and memory bandwidth)[[1]](https://lowyx.com/posts/gt-gpu-M8/#:~:text=,is%20the%20most%20critical%20factor)[[2]](https://www.intel.com/content/www/us/en/docs/advisor/get-started-guide/2023-0/measure-gpu-performance-using-gpu-roofline.html#:~:text=A%20Roofline%20chart%20plots%20an,the%20machine%27s%20maximum%20achievable%20performance). By placing a DNN kernel (e.g. a matrix multiply or attention layer) on a roofline chart, architects can immediately see whether the kernel is compute-bound or memory-bound on a given GPU, revealing the primary performance bottleneck[[1]](https://lowyx.com/posts/gt-gpu-M8/#:~:text=,is%20the%20most%20critical%20factor)[[2]](https://www.intel.com/content/www/us/en/docs/advisor/get-started-guide/2023-0/measure-gpu-performance-using-gpu-roofline.html#:~:text=A%20Roofline%20chart%20plots%20an,the%20machine%27s%20maximum%20achievable%20performance). This helps guide design trade-offs – for instance, if a CNN convolution lies below the memory bandwidth “roof,” adding more ALUs won’t help as much as improving memory throughput.

Other analytical models focus on predicting absolute performance (e.g. latency or throughput) from hardware and workload parameters. A classic example is the model by **Hong & Kim (2009)**, which estimates GPU kernel execution time by calculating the degree of *memory-level parallelism* given the number of active threads and memory bandwidth[[3]](https://www.researchgate.net/publication/220771200_An_analytical_model_for_a_GPU_architecture_with_memory-level_and_thread-level_parallelism_awareness#:~:text=parallel%20applications%20on%20GPU%20architectures%2C,and%20on%20GPU%20computing). This model computes how many memory requests can proceed in parallel (the “memory warp parallelism”) and uses that to predict memory stall time and overall runtime[[3]](https://www.researchgate.net/publication/220771200_An_analytical_model_for_a_GPU_architecture_with_memory-level_and_thread-level_parallelism_awareness#:~:text=parallel%20applications%20on%20GPU%20architectures%2C,and%20on%20GPU%20computing). Such theoretical models, even if simplified, offer insight into how changes in GPU design (like more memory channels, larger caches, or additional warps) might affect performance on DNN layers. They are often used early in the design cycle to narrow down promising design directions (e.g. determining if a larger register file to support more warps would hide memory latency for transformer layers, or if an LLM’s matrix multiplication is already compute-limited by FMA units).

In industry, analytical and empirical modeling are combined to project performance of new designs. GPU vendors use performance counters and profiling on existing hardware to calibrate models for future architectures. For example, developers can profile kernels with tools like NVIDIA Nsight or Intel’s Advisor and then apply a roofline or CPI analysis model to infer how a new architecture might perform[[4]](https://www.researchgate.net/publication/220771200_An_analytical_model_for_a_GPU_architecture_with_memory-level_and_thread-level_parallelism_awareness#:~:text=parameters%20required%20for%20CWP%20depend,). Intel’s Advisor actually automates a roofline analysis for GPUs – it collects hardware metrics and plots each kernel’s performance vs. the GPU’s calculated roofs, highlighting which resource is limiting the kernel[[5]](https://www.intel.com/content/www/us/en/docs/advisor/get-started-guide/2023-0/measure-gpu-performance-using-gpu-roofline.html#:~:text=GPU%20Roofline%20Insights%20perspective%20enables,determine%20the%20main%20limiting%20factor)[[2]](https://www.intel.com/content/www/us/en/docs/advisor/get-started-guide/2023-0/measure-gpu-performance-using-gpu-roofline.html#:~:text=A%20Roofline%20chart%20plots%20an,the%20machine%27s%20maximum%20achievable%20performance). In summary, analytical models (from back-of-the-envelope arithmetic intensity estimates to formal models like Hong & Kim’s) are invaluable for rapid *theoretical predictions* of inference performance and bottlenecks, guiding what to focus on in more detailed simulations.

## Cycle-Accurate Microarchitectural Simulation

While analytical models give high-level guidance, **cycle-level simulators** provide the detailed answers needed to evaluate new GPU designs. These simulators model the GPU’s microarchitecture (SMs/CUs, pipelines, warp schedulers, caches, memory controllers, etc.) and execute instructions cycle by cycle, allowing architects to measure how a hypothetical GPU configuration would perform on real workloads. Simulation-based approaches are the gold standard for identifying subtle bottlenecks and verifying architectural trade-offs, since they capture timing, contention, and parallelism effects that simpler models might miss[[6]](https://research.nvidia.com/publication/2021-02_need-speed-experiences-building-trustworthy-system-level-gpu-simulator#:~:text=development%20of%20NVArchSim%20,modeling%20accuracy%2C%20NVAS%20delivers%20simulation).

Academic researchers and industry engineers have developed several GPU simulators – many open-source – to experiment with new ideas for GPU inference. Notably, **GPGPU-Sim** is a widely used simulator that models NVIDIA-style GPUs at the cycle level, supporting CUDA/PTX or SASS (machine ISA) code execution[[7]](https://lowyx.com/posts/gt-gpu-M8/#:~:text=GPU%20simulators,is%20combining%20Gem5%2C%20the%20most). GPGPU-Sim is *execution-driven*, meaning it actually executes the program’s instructions through the simulated pipeline, producing cycle-by-cycle performance metrics. It has been used extensively in academia to explore GPU architecture changes (e.g. new cache designs, warp scheduling policies) and evaluate their impact on workloads like CNNs or general GPGPU programs[[7]](https://lowyx.com/posts/gt-gpu-M8/#:~:text=GPU%20simulators,is%20combining%20Gem5%2C%20the%20most).

Building on that foundation, **Accel-Sim** is a newer framework (from the same UBC/Georgia Tech collaboration) that extends GPGPU-Sim to support more modern GPU ISAs and workloads with higher fidelity. Accel-Sim introduces a *trace-driven* mode, where it uses pre-collected instruction traces of CUDA kernels to drive the timing simulation[[7]](https://lowyx.com/posts/gt-gpu-M8/#:~:text=GPU%20simulators,is%20combining%20Gem5%2C%20the%20most). By decoupling functional execution from timing, trace-driven simulators can be faster or more flexible in certain studies (e.g. enabling designers to simulate many kernel iterations quickly once the trace is captured). Accel-Sim’s creators report significantly improved accuracy versus earlier simulators by incorporating validated models up through NVIDIA’s Turing generation[[8]](https://dl.acm.org/doi/full/10.1145/3725843.3757107#:~:text=ensures%20the%20sampled%20kernels%20faithfully,represent%20the%20entire%20workload). In fact, Accel-Sim reduced simulation error by dozens of percentage points compared to prior tools across a range of benchmarks[[8]](https://dl.acm.org/doi/full/10.1145/3725843.3757107#:~:text=ensures%20the%20sampled%20kernels%20faithfully,represent%20the%20entire%20workload) – making it a state-of-the-art academic tool for detailed GPU modeling.

Several other open simulators cater to different architectures or research needs:

* **Multi2Sim** – an older but well-known simulator – provides ISA-level simulation of heterogeneous CPU–GPU systems. It originally focused on x86 CPUs and AMD GPUs (Evergreen and Southern Islands families), executing OpenCL or AMD ISA code to evaluate GPU designs[[9]](https://www.scilit.com/publications/f285f68ac4a7a4705f2d5b40aba6b39f#:~:text=Multi2Sim%20,CPU%20and%20an%20AMD). Multi2Sim enabled early GPGPU design-space exploration in academia, such as studying cache coherence or memory controller designs for GPU workloads.
* **MGPUSim** – a modern simulator for AMD GPUs – is explicitly designed to handle *multi-GPU* systems. It models AMD’s GCN3 ISA, and is built on a flexible, event-driven core (the Akita framework) to allow scaling to multiple GPUs[[10]](https://sites.google.com/view/jlabellan/tutorial-mgpusim#:~:text=source%2C%20flexible%2C%20high,when%20compared%20with). MGPUSim executes unmodified OpenCL kernels compiled with ROCm, and has been validated against real Radeon hardware with an error as low as ~5.5%[[10]](https://sites.google.com/view/jlabellan/tutorial-mgpusim#:~:text=source%2C%20flexible%2C%20high,when%20compared%20with). Its key strength is the ability to simulate a heterogeneous multi-GPU platform, including custom inter-GPU networks and memory systems, which is valuable for evaluating large-scale DNN inference (e.g. how an LLM’s layers could be partitioned across several GPUs). Researchers have even integrated a visual profiler (“Daisen”) with MGPUSim to analyze performance bottlenecks (like inter-GPU communication hot spots or load imbalance) in the simulated multi-GPU runs[[11]](https://sites.google.com/view/jlabellan/tutorial-mgpusim#:~:text=participants%20will%20gain%20a%20more,MGPUSim%20to%20support%20their%20research).
* **MacSim** – developed at Georgia Tech – is another cycle-level simulator geared towards heterogeneous architectures. It can simulate CPU cores (x86 or ARM) alongside GPUs, supporting NVIDIA PTX and even Intel GPU instruction traces[[12]](https://lowyx.com/posts/gt-gpu-M8/#:~:text=it%20is%20an%20execution%20driven,In%20this)[[13]](https://hparch.gatech.edu/macsim.html#:~:text=MacSim%20,PTX%2C%20and%20Intel%20GPU%20instructions). MacSim is largely trace-driven and has been used to study CPU–GPU coordination, scheduling, and memory system effects in a unified simulator. Its support for both NVIDIA and Intel GPU ISA traces makes it useful for comparing architectures or exploring vendor-agnostic optimizations in DNN workloads[[12]](https://lowyx.com/posts/gt-gpu-M8/#:~:text=it%20is%20an%20execution%20driven,In%20this).
* **gem5 with GPU models** – The gem5 simulator (a popular open-source CPU architectural simulator) has been extended to support GPU modeling by integrating GPU cores from other simulators. Notably, one version combined gem5 with GPGPU-Sim (“gem5-GPU”), enabling full-system simulation with a CPU, memory system, and a GPU device together[[14]](https://lowyx.com/posts/gt-gpu-M8/#:~:text=GPUs%20are%20supported,In%20this). There have been both AMD-oriented and NVIDIA-oriented gem5 GPU integrations. This kind of framework is useful for system-level studies (e.g. a CPU feeding inference tasks to a GPU, modeling end-to-end latency, or examining OS/driver interactions)[[14]](https://lowyx.com/posts/gt-gpu-M8/#:~:text=GPUs%20are%20supported,In%20this). It executes both CPU and GPU instructions in one unified environment, albeit with a performance penalty given the complexity.

Each of these cycle-accurate simulators serves as a “virtual GPU” where architects can implement hypothetical features (larger caches, new tensor core units, different thread scheduling, etc.) and then *measure* the effect on DNN inference workloads. For example, one could modify GPGPU-Sim to add a special cache for convolution filters and see how much it speeds up ResNet inference, or use MGPUSim to test a new multi-GPU all-reduce network for accelerating ViT model inference. The downside of cycle-level simulation is speed – they are very slow (often executing only thousands of simulated GPU instructions per second). Thus, researchers use representative inputs or shorter traces of DNN workloads when doing detailed simulation. Despite the slowness, these tools are critical for pinpointing microarchitectural bottlenecks (like whether ALUs are under-utilized, or memory throughput is saturating, or a proposed prefetcher actually helps on real layers). They provide deep insight into *why* a certain GPU design performs as it does on inference tasks by literally counting stalls, pipeline hazards, bank conflicts, etc., cycle by cycle.

**Execution-driven vs. trace-driven simulation:** It’s worth noting the distinction between these approaches. Execution-driven simulators (e.g. GPGPU-Sim, MGPUSim by default) run the program’s code and make decisions on-the-fly about control flow and memory accesses, accurately modeling dynamic behaviors like warp divergence and memory coalescing. Trace-driven simulators (e.g. Accel-Sim’s trace mode, MacSim) rely on a pre-recorded trace of instructions or memory events collected from a real run or an execution-driven sim; this can speed up simulation and allows exploring different timing models on the same instruction stream. The choice often comes down to flexibility vs. fidelity – trace-driven methods can miss behaviors not in the trace (like new divergence patterns), but they are great for fast “what-if” evaluation across many hardware configurations using the same workload trace[[7]](https://lowyx.com/posts/gt-gpu-M8/#:~:text=GPU%20simulators,is%20combining%20Gem5%2C%20the%20most). In practice, researchers employ a mix: using execution-driven mode to generate representative traces or to validate that traces cover the important behaviors, then sweeping a large design space in trace-driven mode for efficiency[[7]](https://lowyx.com/posts/gt-gpu-M8/#:~:text=GPU%20simulators,is%20combining%20Gem5%2C%20the%20most).

## System-Level and Inference-Centric Simulation

As DNN models and deployments grow, evaluating GPU designs *in context* – i.e. within a multi-GPU system or at datacenter scale – has become crucial. **System-level inference simulation** methodologies aim to model not just a single GPU’s internals, but the interaction of many GPUs, CPUs, and the surrounding system (interconnect, memory, workload scheduler) when running inference workloads. These approaches help identify bottlenecks that arise at scale: e.g. communication overheads between GPUs, latency impact of batch scheduling, or I/O and memory capacity issues when serving large models.

One class of system-level evaluation uses **distributed event-driven simulators** that incorporate GPU performance models. For example, researchers have used the Structural Simulation Toolkit (SST) and other discrete-event simulators to represent a cluster of nodes, each with a GPU, and simulate how inference requests flow through them. However, more specialized frameworks have emerged specifically for deep learning inference. Recent research has produced tools like **Vidur** and **Frontier** (targeting LLM serving) that operate at a higher level of abstraction: they model each GPU’s performance using simplified models or pre-collected profile data, and simulate the *workflow* of an inference request through possibly multiple stages and devices.

**Vidur** is a large-scale simulation framework for LLM inference that was introduced to help navigate the complex configuration space of deployment options. Instead of simulating every GPU cycle, Vidur profiles a set of key operators (GEMMs, attention, etc.) on an actual GPU and uses those measurements to parameterize a simulator of the end-to-end serving system[[15]](https://medium.com/%40techsachin/vidur-a-large-scale-simulation-framework-for-llm-inference-performance-1006909e6f36#:~:text=device.%20,declarative%20specification%20of%20the%20model)[[16]](https://medium.com/%40techsachin/vidur-a-large-scale-simulation-framework-for-llm-inference-performance-1006909e6f36#:~:text=). It can simulate various parallelization strategies (data parallel, tensor model parallel, pipeline parallel) and scheduling policies with high speed, by leveraging the fact that most LLMs share common building-block operations[[15]](https://medium.com/%40techsachin/vidur-a-large-scale-simulation-framework-for-llm-inference-performance-1006909e6f36#:~:text=device.%20,declarative%20specification%20of%20the%20model). Vidur reportedly achieves latency estimates within ~9% of real measurements for LLM inference and includes an automated search tool to find the best deployment configuration under given constraints[[17]](https://medium.com/%40techsachin/vidur-a-large-scale-simulation-framework-for-llm-inference-performance-1006909e6f36#:~:text=it%20estimates%20inference%20latency%20with,hours%20%E2%80%94%20costing%20218K%20dollars). This kind of framework is extremely useful for systems engineers: for instance, to decide how to split an 70B-parameter model across 8 GPUs and what batch size or scheduler to use, one could use Vidur to simulate many scenarios in software within hours rather than running costly physical experiments (which might take tens of thousands of GPU-hours)[[17]](https://medium.com/%40techsachin/vidur-a-large-scale-simulation-framework-for-llm-inference-performance-1006909e6f36#:~:text=it%20estimates%20inference%20latency%20with,hours%20%E2%80%94%20costing%20218K%20dollars). By observing the simulated throughput/latency and resource utilization, designers can pinpoint system bottlenecks – e.g. noticing that the PCIe communication or KV-cache swapping becomes the limiter for long sequences – and then consider architectural changes (like adding NVLink bandwidth or larger GPU memory) to alleviate those bottlenecks.

Pushing the envelope further, **Frontier** (an open research effort in 2025) is a simulator tailored for next-generation LLM inference paradigms such as *Mixture-of-Experts (MoE) models* and *disaggregated inference*[[18]](https://arxiv.org/html/2508.03148v1#:~:text=Frontier%20introduces%20a%20unified%20framework,To). Frontier introduces new abstractions to model *multi-stage* inference pipelines that span heterogeneous clusters, something previous “replica-based” simulators could not natively represent[[19]](https://arxiv.org/html/2508.03148v1#:~:text=system,community%20to%20design%20and%20optimize)[[20]](https://arxiv.org/html/2508.03148v1#:~:text=complexity%C2%A0,the%20native%20primitives%20to%20represent). For example, in a disaggregated setup one set of GPU servers might handle the “prefill” (embedding and first token) of an LLM query while another set handles the autoregressive decode phase – with a data-dependent feedback loop between them. Frontier can simulate such workflows, including cross-cluster routing of MoE expert tokens and backpressure between stages, to evaluate designs like specialized memory-rich decode accelerators or high-bandwidth interconnects for the attention layers[[18]](https://arxiv.org/html/2508.03148v1#:~:text=Frontier%20introduces%20a%20unified%20framework,To). Although Frontier is still at the research prototype stage, it represents the kind of system-level simulation needed to explore architectural trade-offs in *distributed* GPU inference (e.g. if we separate attention and feed-forward networks onto different GPU types, what network latency and scheduling overhead is tolerable before end-to-end latency suffers?).

Another notable approach in system-level evaluation is the combination of **detailed kernel simulation with higher-level trace simulation**, as exemplified by **ReaLLM**[[21]](https://www.bsg.ai/papers/Peng_ReaLLM_ASAP_2025.pdf#:~:text=constructs%20a%20precomputed%20kernel%20library,modern%20GPU%02based%20LLM%20inference%20is)[[22]](https://www.bsg.ai/papers/Peng_ReaLLM_ASAP_2025.pdf#:~:text=than%20memory,source%20and%20available). ReaLLM (introduced in 2025 by UW researchers) bridges chip-level and system-level modeling for LLM inference by integrating a cycle-accurate GPU simulator for individual kernels with a trace-driven simulator for the overall workload[[23]](https://www.bsg.ai/papers/Peng_ReaLLM_ASAP_2025.pdf#:~:text=ReaLLM%2C%20a%20trace,efficiently%20explore%20a%20vast%20design)[[24]](https://www.bsg.ai/papers/Peng_ReaLLM_ASAP_2025.pdf#:~:text=To%20address%20these%20limitations%2C%20we,as%20matrix%20multiplication%2C%20layer%20normalization). The idea is to pre-compute a library of kernel performance results under various parallelism and batching scenarios (using a detailed GPU model or actual profiling), and then use those as inputs to a fast simulator that can emulate an entire inference run across multiple GPUs and batches[[25]](https://www.bsg.ai/papers/Peng_ReaLLM_ASAP_2025.pdf#:~:text=in%20three%20distinct%20stages,strategies%20for%20each%20kernel%2C%20storing). By doing so, ReaLLM achieves high accuracy – about 9.1% error on end-to-end latency when simulating GPT-style inference on 4 NVIDIA H100 GPUs[[26]](https://www.bsg.ai/papers/Peng_ReaLLM_ASAP_2025.pdf#:~:text=space%20of%20LLM%20inference%20systems,bound%20rather) – while running much faster than a pure cycle-level simulation of the whole thing. In fact, by caching kernel results, it speeds up full-system simulations by 6× and scenario explorations (like trying different scheduling or QoS settings) by 164× vs. not using the precomputed library[[22]](https://www.bsg.ai/papers/Peng_ReaLLM_ASAP_2025.pdf#:~:text=than%20memory,source%20and%20available). ReaLLM’s studies have revealed that at large scales, modern GPU-based LLM inference is increasingly *compute-bound* rather than memory-bound[[27]](https://www.bsg.ai/papers/Peng_ReaLLM_ASAP_2025.pdf#:~:text=average%20end,bound%20rather) – a valuable insight for architects considering whether to beef up memory or add compute units for future GPU designs. Overall, this hybrid methodology represents a *system-level inference simulator* that still retains a connection to low-level hardware details, enabling architects to evaluate how a proposed GPU microarchitectural improvement would translate to real-world performance when deployed in a multi-GPU inference setting.

Finally, **industrial practices** deserve mention. Companies like NVIDIA and AMD also utilize system-level simulation and emulation to evaluate new GPU designs for AI workloads. NVIDIA, for instance, built an internal simulator called **NVArchSim (NVAS)** to guide their GPU development[[28]](https://research.nvidia.com/publication/2021-02_need-speed-experiences-building-trustworthy-system-level-gpu-simulator#:~:text=development%20of%20NVArchSim%20,modeling%20accuracy%2C%20NVAS%20delivers%20simulation). NVArchSim is a **“trustworthy system-level GPU simulator”** used to rapidly experiment with new architectural features in the context of HPC and ML workloads[[28]](https://research.nvidia.com/publication/2021-02_need-speed-experiences-building-trustworthy-system-level-gpu-simulator#:~:text=development%20of%20NVArchSim%20,modeling%20accuracy%2C%20NVAS%20delivers%20simulation). Crucially, NVIDIA found that an overly detailed (but slow) simulator can hinder the design process; instead, NVAS prioritizes speed and reasonable accuracy by modeling only as much detail as needed for good predictions[[28]](https://research.nvidia.com/publication/2021-02_need-speed-experiences-building-trustworthy-system-level-gpu-simulator#:~:text=development%20of%20NVArchSim%20,modeling%20accuracy%2C%20NVAS%20delivers%20simulation). This philosophy – adding fidelity *only* when it meaningfully improves accuracy – allows NVAS to run *orders of magnitude faster* than most academic GPU simulators while still modeling single- and multi-GPU systems with high accuracy[[29]](https://research.nvidia.com/publication/2021-02_need-speed-experiences-building-trustworthy-system-level-gpu-simulator#:~:text=models%20hamper%20an%20architect%27s%20ability,and%20encourage%20those%20in%20academia). Because of this speed, NVIDIA can simulate **hundreds** of real workloads (including full ML models) early in the design cycle, uncovering performance issues or verifying improvements long before silicon exists[[28]](https://research.nvidia.com/publication/2021-02_need-speed-experiences-building-trustworthy-system-level-gpu-simulator#:~:text=development%20of%20NVArchSim%20,modeling%20accuracy%2C%20NVAS%20delivers%20simulation). NVArchSim is not public, but their 2021 HPCA paper highlights that balancing detail vs. speed was key to making the simulator a practical tool for architects[[29]](https://research.nvidia.com/publication/2021-02_need-speed-experiences-building-trustworthy-system-level-gpu-simulator#:~:text=models%20hamper%20an%20architect%27s%20ability,and%20encourage%20those%20in%20academia). Similarly, other GPU vendors and cloud companies likely use internal tools or FPGA-based prototypes to simulate GPU performance on AI inference – often informed by the same principles as the academic tools discussed (e.g. using traces, sampling, and analytical insights to accelerate the evaluation).

In summary, system-level simulation methodologies – whether academic frameworks like Vidur/Frontier/ReaLLM or industry’s in-house simulators – enable *holistic evaluation* of new GPU designs. They capture how an architecture change (e.g. higher memory bandwidth, more tensor cores, or a new scheduling mechanism) plays out not just in a single kernel benchmark, but in a deployed inference scenario with real-world factors like batching delays, multi-GPU communication, and memory swapping. This is increasingly important for evaluating *inference performance* of giant models and for discovering bottlenecks that only appear at scale (for instance, an LLM inference might be limited by network latency between GPUs, indicating a need for better interconnect or model partitioning strategy).

## Notable Open-Source Tools for GPU Design Evaluation

Below is a categorized list of well-known open-source tools used to evaluate GPU architectures on DNN inference workloads, along with their capabilities and typical use cases:

* **GPGPU-Sim** – A cycle-accurate GPU simulator modeling NVIDIA GPUs (supporting CUDA PTX and SASS). It is execution-driven, meaning it runs actual GPU kernels through a simulated pipeline[[7]](https://lowyx.com/posts/gt-gpu-M8/#:~:text=GPU%20simulators,is%20combining%20Gem5%2C%20the%20most). *Use cases:* Academic research on microarchitectural features (caches, SM schedulers, memory systems). GPGPU-Sim is often used to measure the low-level performance impact of new architecture ideas on CUDA workloads, including CNN kernels or basic transformer operations.
* **Accel-Sim** – An extensible simulation framework derived from GPGPU-Sim that supports modern NVIDIA GPU architectures up to Turing/Ampere. It provides both execution-driven and trace-driven modes[[7]](https://lowyx.com/posts/gt-gpu-M8/#:~:text=GPU%20simulators,is%20combining%20Gem5%2C%20the%20most). Accel-Sim includes tools to generate instruction traces from real GPU runs and then simulate those traces with cycle accuracy. *Use cases:* Fast exploration of GPU design alternatives with validated accuracy. For example, architects can collect a trace of a DNN inference workload and quickly test multiple hardware configurations in trace-driven simulations. Its validation against real hardware makes it reliable for performance prediction[[8]](https://dl.acm.org/doi/full/10.1145/3725843.3757107#:~:text=ensures%20the%20sampled%20kernels%20faithfully,represent%20the%20entire%20workload).
* **Multi2Sim** – A modular heterogeneous simulator that can model x86 CPUs and AMD GPUs at the ISA level[[9]](https://www.scilit.com/publications/f285f68ac4a7a4705f2d5b40aba6b39f#:~:text=Multi2Sim%20,CPU%20and%20an%20AMD). It was one of the first open tools to simulate an entire CPU–GPU system, focusing on AMD’s older Evergreen GPU ISA and the OpenCL runtime. *Use cases:* Early-stage design exploration and educational use. Multi2Sim allows testing how an AMD-like GPU executes compute kernels, and studying interactions with a CPU (e.g. driver overheads, kernel launch costs), though its GPU models are now somewhat dated.
* **MGPUSim** – A high-flexibility, cycle-level simulator for **AMD GCN3-based GPUs**, with **multi-GPU** capability[[10]](https://sites.google.com/view/jlabellan/tutorial-mgpusim#:~:text=source%2C%20flexible%2C%20high,when%20compared%20with). Implemented in Go on the Akita simulation framework, MGPUSim can run programs compiled with AMD’s ROCm stack and has been validated within ~5% error of real hardware[[10]](https://sites.google.com/view/jlabellan/tutorial-mgpusim#:~:text=source%2C%20flexible%2C%20high,when%20compared%20with). It supports configuring multiple GPUs and custom interconnect topologies. *Use cases:* Research on scaling inference across multiple GPUs, memory coherence and NUMA effects in multi-GPU systems, and evaluating AMD-specific architectural features. For instance, MGPUSim is ideal for testing how a new GPU cache hierarchy or tiling strategy would perform when 4 GPUs collaboratively run a large CNN or when exchanging data during distributed DNN inference.
* **MacSim** – A trace-driven, cycle-level simulator from Georgia Tech for heterogeneous architectures[[30]](https://lowyx.com/posts/gt-gpu-M8/#:~:text=PTX%2FSASS%20and%20this%20is%20a,In%20this). It supports **CPU+GPU simulation**, handling x86/ARM cores together with GPUs via traces of NVIDIA PTX or even Intel GPU instructions[[13]](https://hparch.gatech.edu/macsim.html#:~:text=MacSim%20,PTX%2C%20and%20Intel%20GPU%20instructions). *Use cases:* Studies of scheduling and resource sharing in systems with CPUs and GPUs. MacSim lets researchers play with how a CPU workload and a GPU kernel might interfere via shared memory systems, or how to optimize task scheduling between host and device – scenarios relevant for real-time DNN inference in embedded systems, for example.
* **gem5 (gem5-GPU)** – An integration of gem5 (full-system simulator) with GPU models (from GPGPU-Sim or custom). Open-source variants exist for NVIDIA GPGPU-Sim integration and for modeling AMD’s HSA APUs[[14]](https://lowyx.com/posts/gt-gpu-M8/#:~:text=GPUs%20are%20supported,In%20this). *Use cases:* Whole-system performance evaluation including OS effects. Using gem5 with a GPU model, one can boot a Linux system with a GPU driver and run inference code to measure end-to-end latency, including CPU overheads and context switches. It’s useful for architects who want to evaluate, say, a new GPU preemption mechanism or the impact of unified memory on inference throughput.
* **ReaLLM** – A **trace-driven inference simulation framework** (open-sourced by the Bespoke Silicon Group in 2025) that **bridges chip-level and system-level modeling**[[23]](https://www.bsg.ai/papers/Peng_ReaLLM_ASAP_2025.pdf#:~:text=ReaLLM%2C%20a%20trace,efficiently%20explore%20a%20vast%20design)[[24]](https://www.bsg.ai/papers/Peng_ReaLLM_ASAP_2025.pdf#:~:text=To%20address%20these%20limitations%2C%20we,as%20matrix%20multiplication%2C%20layer%20normalization). It profiles or simulates key kernels (GEMMs, attention, etc.) in detail to create a performance library, then simulates the *sequence of operations and communication* in an LLM inference with those precomputed costs. It achieves high accuracy (~9% error for multi-GPU LLM serving) at far lower runtime cost than full cycle simulation[[26]](https://www.bsg.ai/papers/Peng_ReaLLM_ASAP_2025.pdf#:~:text=space%20of%20LLM%20inference%20systems,bound%20rather)[[22]](https://www.bsg.ai/papers/Peng_ReaLLM_ASAP_2025.pdf#:~:text=than%20memory,source%20and%20available). *Use cases:* Rapid evaluation of design trade-offs for large-model serving – e.g. testing different parallelism strategies, GPU memory sizes, or network bandwidths to see how they affect end-to-end metrics like latency and throughput for LLM inference. ReaLLM is especially handy for co-designing hardware and scheduling policies, since it can quickly simulate variations once the kernel library is built.
* **Vidur** – An inference performance simulator focused on **large-scale LLM deployments**, introduced by researchers to automate LLM serving optimizations. It uses a combination of analytical modeling and minimal profiling to predict how a given model (like GPT-style networks) will perform under different configurations (batching schemes, parallelism, hardware choices)[[15]](https://medium.com/%40techsachin/vidur-a-large-scale-simulation-framework-for-llm-inference-performance-1006909e6f36#:~:text=device.%20,declarative%20specification%20of%20the%20model)[[16]](https://medium.com/%40techsachin/vidur-a-large-scale-simulation-framework-for-llm-inference-performance-1006909e6f36#:~:text=). It also includes a search tool (“Vidur-Search”) to find cost-effective configurations meeting a target latency. *Use cases:* System-level design space exploration for LLM serving. For example, a cloud provider can use Vidur to decide the cheapest GPU cluster configuration to serve a 70B model under a 100 ms latency requirement, instead of brute-force testing every option on real hardware[[17]](https://medium.com/%40techsachin/vidur-a-large-scale-simulation-framework-for-llm-inference-performance-1006909e6f36#:~:text=it%20estimates%20inference%20latency%20with,hours%20%E2%80%94%20costing%20218K%20dollars). Vidur’s strength is modeling the *runtime dynamics* of LLM inference (like the two-phase prefill/decode behavior and varying sequence lengths) and how those dynamics interact with batching and scheduling choices[[31]](https://medium.com/%40techsachin/vidur-a-large-scale-simulation-framework-for-llm-inference-performance-1006909e6f36#:~:text=ii)[[32]](https://medium.com/%40techsachin/vidur-a-large-scale-simulation-framework-for-llm-inference-performance-1006909e6f36#:~:text=,and%20lead%20to%20aggregate%20errors). (Vidur is research-oriented; if its code is released, it becomes a valuable open tool for the community in this space.)
* **Frontier** – An open research simulator (currently in development) targeting **next-gen inference architectures** such as disaggregated (multi-cluster) deployments and MoE models[[33]](https://arxiv.org/html/2508.03148v1#:~:text=Large%20Language%20Model%20,of%20complex%20workflows%20like%20cross)[[18]](https://arxiv.org/html/2508.03148v1#:~:text=Frontier%20introduces%20a%20unified%20framework,To). Frontier provides a unified event-driven framework to model both traditional co-located GPU servers and novel disaggregated setups with specialized pools for different inference stages[[18]](https://arxiv.org/html/2508.03148v1#:~:text=Frontier%20introduces%20a%20unified%20framework,To). It natively supports MoE expert parallelism and complex pipelines with cross-cluster communication. *Use cases:* Experimental evaluation of cutting-edge serving architectures. If one is considering a design where, say, memory-heavy attention operations run on a separate set of GPU accelerators from compute-intensive feed-forward networks, Frontier can simulate the end-to-end performance and help pinpoint new bottlenecks (like network hot spots or load imbalance among experts) in these unconventional designs[[19]](https://arxiv.org/html/2508.03148v1#:~:text=system,community%20to%20design%20and%20optimize)[[20]](https://arxiv.org/html/2508.03148v1#:~:text=complexity%C2%A0,the%20native%20primitives%20to%20represent). It effectively enables architects to *play out* what-if scenarios for future large-model serving without needing a physical testbed.

Each of these tools contributes to a comprehensive toolkit for GPU design evaluation. By using analytical models, architects obtain quick intuition on bottlenecks and scalability limits. With cycle-level simulators, they gain detailed verification of microarchitectural behavior and can measure the impact of low-level tweaks on DNN kernels. And with system-level and hybrid simulators, they can project those findings onto realistic inference deployments at scale, ensuring that a new GPU design not only performs well in theory, but also translates to real-world efficiency when running large models in production. The combination of methods – from roofline charts to full-stack simulations – is essential for making informed design decisions in the rapidly evolving landscape of AI hardware.

**Sources:** The information above is drawn from a range of academic and industrial publications and documentation, including open-source project repositories and research papers. Key references include the YxLow architecture course notes summarizing GPU simulators[[34]](https://lowyx.com/posts/gt-gpu-M8/#:~:text=GPU%20simulators,In%20this), the NVIDIA Research HPCA 2021 paper on NVArchSim[[28]](https://research.nvidia.com/publication/2021-02_need-speed-experiences-building-trustworthy-system-level-gpu-simulator#:~:text=development%20of%20NVArchSim%20,modeling%20accuracy%2C%20NVAS%20delivers%20simulation), Intel and NVIDIA documentation on roofline and profiling tools[[5]](https://www.intel.com/content/www/us/en/docs/advisor/get-started-guide/2023-0/measure-gpu-performance-using-gpu-roofline.html#:~:text=GPU%20Roofline%20Insights%20perspective%20enables,determine%20the%20main%20limiting%20factor)[[4]](https://www.researchgate.net/publication/220771200_An_analytical_model_for_a_GPU_architecture_with_memory-level_and_thread-level_parallelism_awareness#:~:text=parameters%20required%20for%20CWP%20depend,), and recent computer architecture conference papers (MICRO, ISCA, etc.) introducing tools like Accel-Sim[[8]](https://dl.acm.org/doi/full/10.1145/3725843.3757107#:~:text=ensures%20the%20sampled%20kernels%20faithfully,represent%20the%20entire%20workload), MGPUSim[[10]](https://sites.google.com/view/jlabellan/tutorial-mgpusim#:~:text=source%2C%20flexible%2C%20high,when%20compared%20with), TrioSim/Photon (from SARCHlab), Vidur[[15]](https://medium.com/%40techsachin/vidur-a-large-scale-simulation-framework-for-llm-inference-performance-1006909e6f36#:~:text=device.%20,declarative%20specification%20of%20the%20model), Frontier[[18]](https://arxiv.org/html/2508.03148v1#:~:text=Frontier%20introduces%20a%20unified%20framework,To), and ReaLLM[[26]](https://www.bsg.ai/papers/Peng_ReaLLM_ASAP_2025.pdf#:~:text=space%20of%20LLM%20inference%20systems,bound%20rather). These sources provide deeper technical detail on the capabilities and validation of each tool, as well as case studies of their use in evaluating GPU performance for DNN inference workloads.

[[1]](https://lowyx.com/posts/gt-gpu-M8/#:~:text=,is%20the%20most%20critical%20factor) [[7]](https://lowyx.com/posts/gt-gpu-M8/#:~:text=GPU%20simulators,is%20combining%20Gem5%2C%20the%20most) [[12]](https://lowyx.com/posts/gt-gpu-M8/#:~:text=it%20is%20an%20execution%20driven,In%20this) [[14]](https://lowyx.com/posts/gt-gpu-M8/#:~:text=GPUs%20are%20supported,In%20this) [[30]](https://lowyx.com/posts/gt-gpu-M8/#:~:text=PTX%2FSASS%20and%20this%20is%20a,In%20this) [[34]](https://lowyx.com/posts/gt-gpu-M8/#:~:text=GPU%20simulators,In%20this) Gt Gpu M8 | yxlow

<https://lowyx.com/posts/gt-gpu-M8/>

[[2]](https://www.intel.com/content/www/us/en/docs/advisor/get-started-guide/2023-0/measure-gpu-performance-using-gpu-roofline.html#:~:text=A%20Roofline%20chart%20plots%20an,the%20machine%27s%20maximum%20achievable%20performance) [[5]](https://www.intel.com/content/www/us/en/docs/advisor/get-started-guide/2023-0/measure-gpu-performance-using-gpu-roofline.html#:~:text=GPU%20Roofline%20Insights%20perspective%20enables,determine%20the%20main%20limiting%20factor) Measure GPU Performance Using GPU Roofline

<https://www.intel.com/content/www/us/en/docs/advisor/get-started-guide/2023-0/measure-gpu-performance-using-gpu-roofline.html>

[[3]](https://www.researchgate.net/publication/220771200_An_analytical_model_for_a_GPU_architecture_with_memory-level_and_thread-level_parallelism_awareness#:~:text=parallel%20applications%20on%20GPU%20architectures%2C,and%20on%20GPU%20computing) [[4]](https://www.researchgate.net/publication/220771200_An_analytical_model_for_a_GPU_architecture_with_memory-level_and_thread-level_parallelism_awareness#:~:text=parameters%20required%20for%20CWP%20depend,) An analytical model for a GPU architecture with memory-level and thread-level parallelism awareness

<https://www.researchgate.net/publication/220771200_An_analytical_model_for_a_GPU_architecture_with_memory-level_and_thread-level_parallelism_awareness>

[[6]](https://research.nvidia.com/publication/2021-02_need-speed-experiences-building-trustworthy-system-level-gpu-simulator#:~:text=development%20of%20NVArchSim%20,modeling%20accuracy%2C%20NVAS%20delivers%20simulation) [[28]](https://research.nvidia.com/publication/2021-02_need-speed-experiences-building-trustworthy-system-level-gpu-simulator#:~:text=development%20of%20NVArchSim%20,modeling%20accuracy%2C%20NVAS%20delivers%20simulation) [[29]](https://research.nvidia.com/publication/2021-02_need-speed-experiences-building-trustworthy-system-level-gpu-simulator#:~:text=models%20hamper%20an%20architect%27s%20ability,and%20encourage%20those%20in%20academia) Need for Speed: Experiences Building a Trustworthy System-Level GPU Simulator. | Research

<https://research.nvidia.com/publication/2021-02_need-speed-experiences-building-trustworthy-system-level-gpu-simulator>

[[8]](https://dl.acm.org/doi/full/10.1145/3725843.3757107#:~:text=ensures%20the%20sampled%20kernels%20faithfully,represent%20the%20entire%20workload) Swift and Trustworthy Large-Scale GPU Simulation with Fine ...

<https://dl.acm.org/doi/full/10.1145/3725843.3757107>

[[9]](https://www.scilit.com/publications/f285f68ac4a7a4705f2d5b40aba6b39f#:~:text=Multi2Sim%20,CPU%20and%20an%20AMD) Multi2Sim | Scilit

<https://www.scilit.com/publications/f285f68ac4a7a4705f2d5b40aba6b39f>

[[10]](https://sites.google.com/view/jlabellan/tutorial-mgpusim#:~:text=source%2C%20flexible%2C%20high,when%20compared%20with) [[11]](https://sites.google.com/view/jlabellan/tutorial-mgpusim#:~:text=participants%20will%20gain%20a%20more,MGPUSim%20to%20support%20their%20research) José L. Abellán, Ph.D. - [Tutorial-MGPUSim]

<https://sites.google.com/view/jlabellan/tutorial-mgpusim>

[[13]](https://hparch.gatech.edu/macsim.html#:~:text=MacSim%20,PTX%2C%20and%20Intel%20GPU%20instructions) MacSim - HPArch - Georgia Tech

<https://hparch.gatech.edu/macsim.html>

[[15]](https://medium.com/%40techsachin/vidur-a-large-scale-simulation-framework-for-llm-inference-performance-1006909e6f36#:~:text=device.%20,declarative%20specification%20of%20the%20model) [[16]](https://medium.com/%40techsachin/vidur-a-large-scale-simulation-framework-for-llm-inference-performance-1006909e6f36#:~:text=) [[17]](https://medium.com/%40techsachin/vidur-a-large-scale-simulation-framework-for-llm-inference-performance-1006909e6f36#:~:text=it%20estimates%20inference%20latency%20with,hours%20%E2%80%94%20costing%20218K%20dollars) [[31]](https://medium.com/%40techsachin/vidur-a-large-scale-simulation-framework-for-llm-inference-performance-1006909e6f36#:~:text=ii) [[32]](https://medium.com/%40techsachin/vidur-a-large-scale-simulation-framework-for-llm-inference-performance-1006909e6f36#:~:text=,and%20lead%20to%20aggregate%20errors) Vidur: A Large-Scale Simulation Framework for LLM Inference Performance | by SACHIN KUMAR | Medium

[https://medium.com/@techsachin/vidur-a-large-scale-simulation-framework-for-llm-inference-performance-1006909e6f36](https://medium.com/%40techsachin/vidur-a-large-scale-simulation-framework-for-llm-inference-performance-1006909e6f36)

[[18]](https://arxiv.org/html/2508.03148v1#:~:text=Frontier%20introduces%20a%20unified%20framework,To) [[19]](https://arxiv.org/html/2508.03148v1#:~:text=system,community%20to%20design%20and%20optimize) [[20]](https://arxiv.org/html/2508.03148v1#:~:text=complexity%C2%A0,the%20native%20primitives%20to%20represent) [[33]](https://arxiv.org/html/2508.03148v1#:~:text=Large%20Language%20Model%20,of%20complex%20workflows%20like%20cross) Frontier: Simulating the Next Generation of LLM Inference Systems

<https://arxiv.org/html/2508.03148v1>

[[21]](https://www.bsg.ai/papers/Peng_ReaLLM_ASAP_2025.pdf#:~:text=constructs%20a%20precomputed%20kernel%20library,modern%20GPU%02based%20LLM%20inference%20is) [[22]](https://www.bsg.ai/papers/Peng_ReaLLM_ASAP_2025.pdf#:~:text=than%20memory,source%20and%20available) [[23]](https://www.bsg.ai/papers/Peng_ReaLLM_ASAP_2025.pdf#:~:text=ReaLLM%2C%20a%20trace,efficiently%20explore%20a%20vast%20design) [[24]](https://www.bsg.ai/papers/Peng_ReaLLM_ASAP_2025.pdf#:~:text=To%20address%20these%20limitations%2C%20we,as%20matrix%20multiplication%2C%20layer%20normalization) [[25]](https://www.bsg.ai/papers/Peng_ReaLLM_ASAP_2025.pdf#:~:text=in%20three%20distinct%20stages,strategies%20for%20each%20kernel%2C%20storing) [[26]](https://www.bsg.ai/papers/Peng_ReaLLM_ASAP_2025.pdf#:~:text=space%20of%20LLM%20inference%20systems,bound%20rather) [[27]](https://www.bsg.ai/papers/Peng_ReaLLM_ASAP_2025.pdf#:~:text=average%20end,bound%20rather) bsg.ai

<https://www.bsg.ai/papers/Peng_ReaLLM_ASAP_2025.pdf>